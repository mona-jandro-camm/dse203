{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48784d3f-c41e-4908-82fe-262828bdbfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim\n",
    "#!pip install pyLDAvis\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4038d708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy as sal\n",
    "from sqlalchemy import text\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbe74e6b-00e2-41ad-88ae-b12d0884fa04",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "905458e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# If trouble importing spacy try:\n",
    "# 1. Kill Jupyter Lab/Jupyter Notebook completely\n",
    "# 2. Go to terminal and type\n",
    "#    export KMP_DUPLICATE_LIB_OK=TRUE\n",
    "# 3. Restart Jupyter and try the import again.\n",
    "\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c18606",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sal.create_engine(\n",
    "    \"postgresql+psycopg2://ag_class:WUcgdfQ1@awesome-hw.sdsc.edu/postgres\"\n",
    ")\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaa8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema query\n",
    "sqlquery = text(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "   table_name,\n",
    "   column_name,\n",
    "   data_type\n",
    "FROM\n",
    "   information_schema.columns\n",
    "WHERE\n",
    "   table_name = 'usnewspaper';\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "result = conn.execute(sqlquery)\n",
    "\n",
    "data = [i for i in result]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e8458f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = text(\n",
    "    \"\"\"\n",
    "SELECT keyword, news, title, publishdate, src\n",
    "    FROM (\n",
    "        SELECT\n",
    "            UNNEST(keywords) AS keyword, news, title, publishdate, src\n",
    "        FROM\n",
    "            usnewspaper\n",
    "        ) AS k\n",
    "WHERE keyword ILIKE 'CTE' OR keyword ILIKE 'encephalopathy';\n",
    "\"\"\"\n",
    ")\n",
    "result = conn.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b85fd9c-73d0-455b-94c3-73c8836addef",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = text(\n",
    "\"\"\"SELECT DISTINCT title, news, keywords \n",
    "    FROM usnewspaper \n",
    "    WHERE ARRAY['cte','lawsuit']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords  \n",
    "     FROM usnewspaper \n",
    "     WHERE ARRAY['nfl', 'helmet']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords  \n",
    "     FROM usnewspaper \n",
    "     WHERE ARRAY['nfl', 'brain']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords \n",
    "    FROM usnewspaper \n",
    "    WHERE ARRAY['encephalopathy']::text[] <@ keywords AND news is not null;\"\"\"\n",
    ")   \n",
    "result = conn.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c1dc42",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed76cbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23283da-8bb5-4b1a-8994-69318b735ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44725c7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR this\n",
    "\n",
    "# sql_query = text(\n",
    "#     \"\"\"\n",
    "# SELECT keyword, news, title, publishdate, src\n",
    "#     FROM (\n",
    "#         SELECT\n",
    "#             UNNEST(keywords) AS keyword, news, title, publishdate, src\n",
    "#         FROM\n",
    "#             usnewspaper\n",
    "#         ) AS k\n",
    "# WHERE keyword ILIKE ANY (ARRAY['CTE', 'encephalopathy']);\n",
    "# \"\"\"\n",
    "# )\n",
    "# result = conn.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d454be5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2 = [i for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54eec736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead9863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR this\n",
    "\n",
    "# sql_query = text(\n",
    "#     \"\"\"\n",
    "# SELECT\n",
    "#     keywords, news, title, publishdate, src\n",
    "# FROM usnewspaper\n",
    "# WHERE keywords && (ARRAY['CTE', 'encephalopathy', 'cte', 'Encephalopathy']);\n",
    "# \"\"\"\n",
    "# )\n",
    "# result = conn.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6c449a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data3 = [i for i in result]\n",
    "# len(data3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4052ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#df = pd.DataFrame(data)\n",
    "df = pd.DataFrame(data, columns=[\"title\", \"news\", \"keywords\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "669f0ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c402e35d-702f-456b-94dd-430e26d09d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!python3 -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8be6bc",
   "metadata": {},
   "source": [
    "## Perform Named Entity Recognition (NER) Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfb85cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4603bf7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp_spacy.pipe(df[\"news\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b8e26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ents = []\n",
    "for doc in docs:\n",
    "    list_of_ents.append(\n",
    "        list(set([ent.text for ent in doc.ents if (ent.label_ == \"ORG\") or (ent.label_ == \"PERSON\")]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9caadfba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"named_entities\"] = list_of_ents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f529032",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79bd286",
   "metadata": {},
   "source": [
    "## Now Perform LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0897f4ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e9c55b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.news.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub(r\"\\S*@\\S*\\s?\", \"\", sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub(r\"\\s+\", \" \", sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"'\", \"\", sent) for sent in data]\n",
    "data = [re.sub(\"`\", \"\", sent) for sent in data]\n",
    "data = [re.sub(\"Â´\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e10d540",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (\n",
    "            gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "        )  # deacc=True removes punctuations\n",
    "\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546e5210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def bigrams_and_trigrams(texts):\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 2 times or more).\n",
    "    bigram = gensim.models.Phrases(texts, min_count=2)\n",
    "    for idx in range(len(texts)):\n",
    "        for token in bigram[texts[idx]]:\n",
    "            if \"_\" in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                texts[idx].append(token)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea6074d-ac06-4129-be91-5bd4d5c16699",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(\n",
    "    data_words_nostops, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    ")\n",
    "\n",
    "data_bigrams = bigrams_and_trigrams(data_lemmatized)\n",
    "data_trigrams = bigrams_and_trigrams(data_bigrams)\n",
    "# print(data_lemmatized[0])\n",
    "# print(data_bigrams[0])\n",
    "print(data_trigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb321b7a-a895-46bc-b096-eab88487667a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Frequency list\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca40dc37-7397-4768-9138-b95e4d24ce00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=4,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha=\"auto\",\n",
    "    per_word_topics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08c17da7-34be-458c-b447-57582a6ee5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc39fb1-21f9-497a-8485-058405f7f81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e371d07b-5523-4a7f-a6c0-f37bd6f99b38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print(\"\\nPerplexity: \", lda_model.log_perplexity(corpus))\n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence=\"c_v\"\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(\"\\nCoherence Score: \", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8fe39b-8a32-448b-aaf5-15f0d64f318b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
