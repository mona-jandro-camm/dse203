{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e3fc0a7-3976-4659-aba4-4b2b4863ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------------------------------\n",
    "#   Author Name:        Camm Perera, Alejandro Hohmann, Ramona Henry\n",
    "#   Create Date:        12-06-2022\n",
    "#   Description:        DSE-203 - Group #5, NFL-CTE Knowledge Graph\n",
    "#   System specs:\n",
    "#        MacOS Monterey   : 12.5.1\n",
    "#        Python           : 3.8.13\n",
    "#        IPython          : 8.4.0\n",
    "#        ipykernel        : 6.15.2\n",
    "#        ipywidgets       : 7.6.5\n",
    "#        jupyter_client   : 6.1.12\n",
    "#        jupyter_core     : 4.10.0\n",
    "#        jupyter_server   : 1.18.1\n",
    "#        jupyterlab       : 3.4.4\n",
    "#        nbclient         : 0.5.13\n",
    "#        nbconvert        : 6.4.4\n",
    "#        nbformat         : 5.5.0\n",
    "#        notebook         : 6.4.12\n",
    "#        qtconsole        : 5.3.2\n",
    "#        traitlets        : 5.1.1\n",
    "# #---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "035f370a-a03c-4eab-b7cd-5dc6d0e30fe4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting stanza\n",
      "  Downloading stanza-1.4.2-py3-none-any.whl (691 kB)\n",
      "\u001b[K     |████████████████████████████████| 691 kB 3.9 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests in /Users/galore/anaconda3/lib/python3.8/site-packages (from stanza) (2.25.1)\n",
      "Requirement already satisfied: numpy in /Users/galore/anaconda3/lib/python3.8/site-packages (from stanza) (1.22.4)\n",
      "Requirement already satisfied: six in /Users/galore/anaconda3/lib/python3.8/site-packages (from stanza) (1.15.0)\n",
      "Collecting protobuf\n",
      "  Downloading protobuf-4.21.11-cp37-abi3-macosx_10_9_universal2.whl (486 kB)\n",
      "\u001b[K     |████████████████████████████████| 486 kB 21.5 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting emoji\n",
      "  Downloading emoji-2.2.0.tar.gz (240 kB)\n",
      "\u001b[K     |████████████████████████████████| 240 kB 51.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: tqdm in /Users/galore/anaconda3/lib/python3.8/site-packages (from stanza) (4.59.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in /Users/galore/anaconda3/lib/python3.8/site-packages (from stanza) (1.7.1)\n",
      "Requirement already satisfied: typing_extensions in /Users/galore/anaconda3/lib/python3.8/site-packages (from torch>=1.3.0->stanza) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/galore/anaconda3/lib/python3.8/site-packages (from requests->stanza) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/galore/anaconda3/lib/python3.8/site-packages (from requests->stanza) (2020.12.5)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /Users/galore/anaconda3/lib/python3.8/site-packages (from requests->stanza) (2.10)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in /Users/galore/anaconda3/lib/python3.8/site-packages (from requests->stanza) (4.0.0)\n",
      "Building wheels for collected packages: emoji\n",
      "  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for emoji: filename=emoji-2.2.0-py3-none-any.whl size=234906 sha256=c3d02df13a5e70fd0b5ec27904e302d6d10ea7e771ddf221be663f8861ab621f\n",
      "  Stored in directory: /Users/galore/Library/Caches/pip/wheels/86/62/9e/a6b27a681abcde69970dbc0326ff51955f3beac72f15696984\n",
      "Successfully built emoji\n",
      "Installing collected packages: protobuf, emoji, stanza\n",
      "Successfully installed emoji-2.2.0 protobuf-4.21.11 stanza-1.4.2\n"
     ]
    }
   ],
   "source": [
    "#!pip install wikipedia\n",
    "!pip install stanza\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5341ac0f-55b8-424d-a30f-a4434ac2ff3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galore/anaconda3/lib/python3.8/site-packages/h5py/__init__.py:46: DeprecationWarning: `np.typeDict` is a deprecated alias for `np.sctypeDict`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import py_stringmatching as sm\n",
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, math, time\n",
    "import wikipedia\n",
    "import stanza\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "import sqlalchemy as sal\n",
    "from sqlalchemy import text\n",
    "\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "\n",
    "# nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a63ae-9c86-46fb-bf8c-4518ff397863",
   "metadata": {},
   "source": [
    "### Load  Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18e7fb67-cbe3-45c4-a922-07de881bd62c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galore/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n",
      "Metadata file is not present in the given path; proceeding to read the csv file.\n"
     ]
    }
   ],
   "source": [
    "basic_stats_df = em.read_csv_metadata(\"../../Datasets/Basic_Stats.csv\", key=\"Player Id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e986dbf-3de9-4744-a59e-5f7a4dbd39e2",
   "metadata": {},
   "source": [
    "### Extract Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "42a1464c-bb65-44c5-92b5-92885a10f579",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galore/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    }
   ],
   "source": [
    "wiki_title = \"List of NFL players with chronic traumatic encephalopathy\"\n",
    "wiki_url = \"https://en.wikipedia.org/wiki/List_of_NFL_players_with_chronic_traumatic_encephalopathy\"\n",
    "\n",
    "# Python Wikipedia library\n",
    "wiki_page_object = wikipedia.page(wiki_title)\n",
    "\n",
    "# Python Beautiful Soup\n",
    "wiki_page = requests.get(wiki_url)\n",
    "soup = BeautifulSoup(wiki_page.content, \"lxml\")\n",
    "\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01def4c0-d489-41c4-b2e9-d7d7ef27d4fb",
   "metadata": {},
   "source": [
    "#### Stanza - stanford NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e1f82d67-065a-49d6-b076-527f3b388d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/galore/anaconda3/lib/python3.8/site-packages/ipykernel/ipkernel.py:287: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
      "  and should_run_async(code)\n"
     ]
    },
    {
     "ename": "ResourcesFileNotFoundError",
     "evalue": "Resources file not found at: /Users/galore/stanza_resources/resources.json  Try to download the model again.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourcesFileNotFoundError\u001b[0m                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-067c8bbf98a1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m nlp = stanza.Pipeline(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"en\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mprocessors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"tokenize,mwt,ner\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0muse_gpu\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpos_batch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/stanza/pipeline/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang, dir, package, processors, logging_level, verbose, use_gpu, model_dir, download_method, resources_url, resources_branch, resources_version, proxies, **kwargs)\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0;31m# Load resources.json to obtain latest packages.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading resource file...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 214\u001b[0;31m         \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_resources_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    215\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlang\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;34m'alias'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mresources\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/stanza/resources/common.py\u001b[0m in \u001b[0;36mload_resources_json\u001b[0;34m(model_dir)\u001b[0m\n\u001b[1;32m    453\u001b[0m     \u001b[0mresources_filepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'resources.json'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mResourcesFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources_filepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresources_filepath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m         \u001b[0mresources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourcesFileNotFoundError\u001b[0m: Resources file not found at: /Users/galore/stanza_resources/resources.json  Try to download the model again."
     ]
    }
   ],
   "source": [
    "nlp = stanza.Pipeline(\n",
    "    \"en\",\n",
    "    processors=\"tokenize,mwt,ner\",\n",
    "    use_gpu=False,\n",
    "    pos_batch_size=3000,\n",
    "    download_method=None,\n",
    ")  # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b18b5d-7aa5-43c7-a7d4-82781860b89c",
   "metadata": {},
   "source": [
    "#### Process Players Affected Wiki Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9f861-e9ad-4a36-8b06-84265ad99fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "players_affected_ls = []\n",
    "\n",
    "# Wiki-Extract Players affected\n",
    "players_affected_ls = soup.select(\"p\")[4:8]\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create lists to hold Person lists\n",
    "affected_players_ls = []\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "for index in players_affected_ls:\n",
    "    doc = nlp(str(index))\n",
    "\n",
    "    # Extract PERSON & ORG entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.type == \"PERSON\":\n",
    "            clean_name = re.split(\"</a\", ent.text)[0]\n",
    "            affected_players_ls.append(clean_name)\n",
    "\n",
    "# Dedupe list contents\n",
    "affected_players_ls = [*set(affected_players_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"# of person: {len(affected_players_ls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6315cc-df6f-489d-a165-0d87b5277ee7",
   "metadata": {},
   "source": [
    "#### Process Former Players affected with CTE Wiki Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77086550-6b7e-43e9-9176-e9aa8b2d5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "former_players_post_mortem_ls = []\n",
    "pm_former_players_ls = []\n",
    "\n",
    "# Wiki-Extract Former players with CTE confirmed post-mortem\n",
    "results = soup.select(\"ul\")[1]\n",
    "former_players_post_mortem_ls = results.find_all(\"a\")\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "for index in former_players_post_mortem_ls:\n",
    "    doc = nlp(str(index))\n",
    "\n",
    "    # Extract PERSON entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.type == \"PERSON\":\n",
    "            clean_name = re.split(\"</a\", ent.text)[0]\n",
    "            pm_former_players_ls.append(clean_name)\n",
    "\n",
    "# Dedupe list contents\n",
    "pm_former_players_ls = [*set(pm_former_players_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"# of person: {len(pm_former_players_ls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7a807-ba48-4bf9-8a3d-5d5174890250",
   "metadata": {},
   "source": [
    "#### Process Deceased players suspected of having had CTE Wiki Sction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5e522-7f5f-42f0-a37d-e722d02368b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "deceased_players_ls = []\n",
    "suspected_deceased_players_ls = []\n",
    "\n",
    "# Wiki-Extract Former players with CTE confirmed post-mortem\n",
    "results = soup.select(\"ul\")[2]\n",
    "deceased_players_ls = results.find_all(\"a\")\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "for index in deceased_players_ls:\n",
    "    doc = nlp(str(index))\n",
    "\n",
    "    # Extract PERSON entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.type == \"PERSON\":\n",
    "            clean_name = re.split(\"</a\", ent.text)[0]\n",
    "            suspected_deceased_players_ls.append(clean_name)\n",
    "\n",
    "# Dedupe list contents\n",
    "suspected_deceased_players_ls = [*set(suspected_deceased_players_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"# of person: {len(suspected_deceased_players_ls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94465ac-5964-4415-b9a2-fbf7442cf5c2",
   "metadata": {},
   "source": [
    "#### Process Living former players diagnosed with CTE or ALS or reporting symptoms consistent with CTE or ALS Wiki Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44e7fe-5d4a-4aa7-a8a0-41b5f86b6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "former_cte_als_players_ls = []\n",
    "cte_als_former_players_ls = []\n",
    "\n",
    "# Wiki-Extract Former players with CTE confirmed post-mortem\n",
    "results = soup.select(\"ul\")[3]\n",
    "former_cte_als_players_ls = results.find_all(\"a\")\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "for index in former_cte_als_players_ls:\n",
    "    doc = nlp(str(index))\n",
    "\n",
    "    # Extract PERSON entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.type == \"PERSON\":\n",
    "            clean_name = re.split(\"</a\", ent.text)[0]\n",
    "            cte_als_former_players_ls.append(clean_name)\n",
    "\n",
    "# Dedupe list contents\n",
    "cte_als_former_players_ls = [*set(cte_als_former_players_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"# of person: {len(cte_als_former_players_ls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8dbeae-dcb7-42e7-ab30-86e561572fb0",
   "metadata": {},
   "source": [
    "#### Process Former players listed as plaintiffs in lawsuits against the NFL for concussion-related injuries received after Wiki playing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993325cd-c0d9-4fef-aea9-22e509a7bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "players_lawsuits_nfl_ls = []\n",
    "players_nfl_lawsuits_ls = []\n",
    "\n",
    "# Wiki-Extract Former players with CTE confirmed post-mortem\n",
    "results = soup.select(\"ul\")[4]\n",
    "players_lawsuits_nfl_ls = results.find_all(\"a\")\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "\n",
    "# When there are many texts, creating all of the stanza docs at once is faster\n",
    "docs_in = [stanza.Document([], text=str(d)) for d in players_lawsuits_nfl_ls]\n",
    "docs_out = nlp(docs_in)\n",
    "\n",
    "for doc in docs_out:\n",
    "    # Extract PERSON & ORG entities\n",
    "    for ent in doc.ents:\n",
    "        if ent.type == \"PERSON\":\n",
    "            clean_name = re.split(\"</a\", ent.text)[0]\n",
    "            players_nfl_lawsuits_ls.append(clean_name)\n",
    "\n",
    "# for doc in docs_out:\n",
    "#     # Extract PERSON & ORG entities\n",
    "#     for ent in doc.ents:\n",
    "#         if (ent.type =='PERSON'):\n",
    "#             clean_name = re.split('', ent.text)[0]\n",
    "#             players_nfl_lawsuits_ls.append(clean_name)\n",
    "\n",
    "# for index in players_lawsuits_nfl_ls:\n",
    "#     doc = nlp(str(index))\n",
    "#\n",
    "#     # Extract PERSON entities\n",
    "#     for ent in doc.ents:\n",
    "#         if (ent.type =='PERSON'):\n",
    "#             clean_name = re.split('</a', ent.text)[0]\n",
    "#             players_nfl_lawsuits_ls.append(clean_name)\n",
    "#\n",
    "# Dedupe list contents\n",
    "players_nfl_lawsuits_ls = [*set(players_nfl_lawsuits_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"# of person: {len(players_nfl_lawsuits_ls)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7386e-f4d0-4392-a9b6-b7a79f1b06c4",
   "metadata": {},
   "source": [
    "### Text Normalization & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3297875-3f0a-4671-818b-f4136b3393d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Normalize player \"Name\" in Kaggle basic stats\n",
    "# ---------------------------------------------------\n",
    "basic_stats_df[\"Clean_Name\"] = (\n",
    "    basic_stats_df.Name.str.lower()\n",
    "    .map(lambda s: s.split()[1] + \" \" + s.split()[0])\n",
    "    .replace(\"[^\\w\\s]\", \" \", regex=True)\n",
    "    .str.strip()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229216f-eaa1-4339-ba80-c50ccb7976ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Remove punctuations & lower name\n",
    "# ---------------------------------------------------\n",
    "def remove_punc(name):\n",
    "    punc = \"\"\"!()-[]{};:'\"\\, <>./?@#$%^&*_~\"\"\"\n",
    "    for ele in name:\n",
    "        if ele in punc:\n",
    "            names = name.replace(ele, \" \")\n",
    "    return name.lower().strip()\n",
    "\n",
    "\n",
    "affected_players_ls = [remove_punc(i) for i in affected_players_ls]\n",
    "pm_former_players_ls = [remove_punc(i) for i in pm_former_players_ls]\n",
    "suspected_deceased_players_ls = [remove_punc(i) for i in suspected_deceased_players_ls]\n",
    "cte_als_former_players_ls = [remove_punc(i) for i in cte_als_former_players_ls]\n",
    "players_nfl_lawsuits_ls = [remove_punc(i) for i in players_nfl_lawsuits_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023ce85-64c9-4e13-8ee4-8f3c4ecdfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Create combo dataframe for each list(above) category\n",
    "# --------------------------------------------------------\n",
    "affected_players_df = pd.DataFrame(\n",
    "    data=[[\"affected_players\"] * len(affected_players_ls), affected_players_ls]\n",
    ").T\n",
    "affected_players_df.columns = [\"cte_category\", \"Clean_Name\"]\n",
    "\n",
    "pm_former_players_df = pd.DataFrame(\n",
    "    data=[[\"pm_former_players\"] * len(pm_former_players_ls), pm_former_players_ls]\n",
    ").T\n",
    "pm_former_players_df.columns = [\"cte_category\", \"Clean_Name\"]\n",
    "\n",
    "suspected_deceased_players_df = pd.DataFrame(\n",
    "    data=[\n",
    "        [\"suspected_deceased_players\"] * len(suspected_deceased_players_ls),\n",
    "        suspected_deceased_players_ls,\n",
    "    ]\n",
    ").T\n",
    "suspected_deceased_players_df.columns = [\"cte_category\", \"Clean_Name\"]\n",
    "\n",
    "cte_als_former_players_df = pd.DataFrame(\n",
    "    data=[\n",
    "        [\"cte_als_former_players\"] * len(cte_als_former_players_ls),\n",
    "        cte_als_former_players_ls,\n",
    "    ]\n",
    ").T\n",
    "cte_als_former_players_df.columns = [\"cte_category\", \"Clean_Name\"]\n",
    "\n",
    "players_nfl_lawsuits_df = pd.DataFrame(\n",
    "    data=[\n",
    "        [\"players_nfl_lawsuits\"] * len(players_nfl_lawsuits_ls),\n",
    "        players_nfl_lawsuits_ls,\n",
    "    ]\n",
    ").T\n",
    "players_nfl_lawsuits_df.columns = [\"cte_category\", \"Clean_Name\"]\n",
    "\n",
    "# Combine dataframes\n",
    "frames = [\n",
    "    affected_players_df,\n",
    "    pm_former_players_df,\n",
    "    suspected_deceased_players_df,\n",
    "    cte_als_former_players_df,\n",
    "    players_nfl_lawsuits_df,\n",
    "]\n",
    "wiki_cte_players_df = pd.concat(frames)\n",
    "wiki_cte_players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c7c76-1b12-43fe-9f3c-c773c0af1489",
   "metadata": {},
   "source": [
    "#### Create CSV file and em.DataFrame for Entity Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ea03b-a3bf-432a-a0fc-ceb5454b32e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create CSV & entity match dataframe for blocking\n",
    "wiki_cte_players_df[\"rec_id\"] = range(1, 1 + len(wiki_cte_players_df))\n",
    "wiki_cte_players_df.to_csv(\"./wiki_cte_players_df.csv\")\n",
    "wiki_person_df = em.read_csv_metadata(\"./wiki_cte_players_df.csv\", key=\"rec_id\")\n",
    "wiki_person_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1f507-b5ce-499a-9f54-afb164425da7",
   "metadata": {},
   "source": [
    "#### Block DataFrames to get Candidate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a0920-95f1-4662-a925-4f425ff7f6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Instantiate blocker objects:\n",
    "# ------------------------------\n",
    "# Create overlap blocker\n",
    "ob = em.OverlapBlocker()\n",
    "\n",
    "# Create attribute equivalence blocker\n",
    "ab = em.AttrEquivalenceBlocker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3eac2-0878-4ea3-9c92-74348c7664d7",
   "metadata": {},
   "source": [
    "#### i. Overlap Block by 'player_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefd457-13e7-41d6-bc3a-5298648f136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Block tables using full name\n",
    "ob_fullname_cand = ob.block_tables(\n",
    "    basic_stats_df,\n",
    "    wiki_person_df,\n",
    "    \"Clean_Name\",\n",
    "    \"Clean_Name\",\n",
    "    allow_missing=False,\n",
    "    l_output_attrs=[\n",
    "        \"Player Id\",\n",
    "        \"Name\",\n",
    "        \"Age\",\n",
    "        \"Current Status\",\n",
    "        \"Birthday\",\n",
    "        \"College\",\n",
    "        \"High School\",\n",
    "        \"Clean_Name\",\n",
    "    ],\n",
    "    r_output_attrs=[\"rec_id\", \"Clean_Name\", \"cte_category\"],\n",
    "    overlap_size=2,\n",
    "    verbose=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b4f4c-bdd9-43b2-b95e-dc836db26c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Matched candidates - Kaggle vs. Wiki page\n",
    "ob_fullname_cand.to_csv(\"./OB_names_matched.csv\")\n",
    "ob_fullname_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c77006-b8ec-46a9-a79d-3cfd163962c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug blocker output : (FOR TESTING ONLY)\n",
    "# #  Unmatched candidates - Kaggle vs. Wiki page\n",
    "# corres = [('Clean_Name', 'Clean_Name')]\n",
    "# ob_fullname_debug = em.debug_blocker(ob_fullname_cand, basic_stats_df, wiki_person_df, output_size=500, attr_corres=corres)\n",
    "\n",
    "# # Display first few tuple pairs from the debug_blocker's output\n",
    "# ob_fullname_debug  #.to_csv('./names_debug.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ac10c-a0d2-4f05-adcf-51b8c295789d",
   "metadata": {},
   "source": [
    "#### ii. Attribute Block by 'player_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff23fd-66e9-4b4b-ac70-430c1148f2b2",
   "metadata": {},
   "source": [
    "### <font color='red'> *** BETTER RESULTS THAN OVERLAP BLOCK ***</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581e6d3-9993-467a-88e7-21309bddb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block using 'full_name_dob' attribute\n",
    "ab_fullname_cand = ab.block_tables(\n",
    "    basic_stats_df,\n",
    "    wiki_person_df,\n",
    "    \"Clean_Name\",\n",
    "    \"Clean_Name\",\n",
    "    allow_missing=False,\n",
    "    l_output_attrs=[\n",
    "        \"Player Id\",\n",
    "        \"Name\",\n",
    "        \"Age\",\n",
    "        \"Current Status\",\n",
    "        \"Birthday\",\n",
    "        \"College\",\n",
    "        \"High School\",\n",
    "        \"Clean_Name\",\n",
    "    ],\n",
    "    r_output_attrs=[\"rec_id\", \"Clean_Name\", \"cte_category\"],\n",
    "    n_jobs=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcae0b-b8ea-4b08-9b9e-6976b73b8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Distinct matched candidates - Kaggle vs. Wiki page\n",
    "ab_fullname_cand.groupby(\"ltable_Player Id\").first().to_csv(\"./AB_names_matched.csv\")\n",
    "ab_fullname_cand.groupby(\"ltable_Player Id\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae542f-2980-4c8a-a1a0-6a3b5f9c02a1",
   "metadata": {},
   "source": [
    "#### Process Organizations from Wiki Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ac3bc-6eb0-4dca-b5fc-08028b13c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def RemoveStopWords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return \" \".join(filtered_sentence)\n",
    "\n",
    "\n",
    "# Remove numbers\n",
    "def RemoveNumbers(text):\n",
    "    return re.sub(r\"\\d+\", \"\", text)\n",
    "\n",
    "\n",
    "# Remove Punctuations\n",
    "def RemovePunctuations(text):\n",
    "    # return re.sub(rf\"[{string.punctuation}]\", \" \", text)\n",
    "    return re.sub(r\"[^\\w\\s]\", \" \", text)\n",
    "\n",
    "\n",
    "# Normalize text\n",
    "def NormalizeText(text):\n",
    "    result = text\n",
    "    # result = RemoveNumbers(result)      # Remove any numbers\n",
    "    result = RemovePunctuations(result)  # Remove any punctuations\n",
    "    result = RemoveStopWords(result)  # Remove stop words\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2838a7-e6f9-430c-83c1-3e19732dc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store wiki orgaization names\n",
    "wiki_org_ls = []\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "doc = nlp(NormalizeText(wiki_page_object.content))\n",
    "\n",
    "# Extract ORG entities\n",
    "for ent in doc.ents:\n",
    "    if ent.type == \"ORG\":\n",
    "        wiki_org_ls.append(ent.text)\n",
    "\n",
    "# Dedupe list contents\n",
    "wiki_org_ls = [*set(wiki_org_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time))\n",
    "print(f\"# of organizations: {len(wiki_org_ls)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cee37-e91b-4f4b-8d4f-09a1f18af807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Wiki page Organizations\n",
    "wiki_org_df = pd.DataFrame(data=wiki_org_ls, columns=[\"wiki_org_name\"])\n",
    "wiki_org_df[\"ord_id\"] = wiki_org_df.index + 1\n",
    "wiki_org_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a5695-11aa-4d4e-8e5b-491d5488971d",
   "metadata": {},
   "source": [
    "#### Assign Parent Node & Direction to Players DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18a32c-45e2-4e53-9b6d-5fcfaf8d7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_fullname_cand[\"parent\"] = [\n",
    "    wiki_org_df.query(\"wiki_org_name == 'NFL'\")[\"wiki_org_name\"].values[0]\n",
    "] * len(ab_fullname_cand)\n",
    "ab_fullname_cand[\"direction\"] = [\"parent_to_child\"] * len(ab_fullname_cand)\n",
    "ab_fullname_cand[\"ltable_Age\"] = pd.to_numeric(\n",
    "    ab_fullname_cand.ltable_Age, downcast=\"integer\"\n",
    ")\n",
    "ab_fullname_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db985d6d-2b6b-491f-993b-329b2b7577ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_fullname_cand.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d682f",
   "metadata": {},
   "source": [
    "## Newspaper Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sal.create_engine(\n",
    "    \"postgresql+psycopg2://ag_class:WUcgdfQ1@awesome-hw.sdsc.edu/postgres\"\n",
    ")\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema query\n",
    "sqlquery = text(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "   table_name,\n",
    "   column_name,\n",
    "   data_type\n",
    "FROM\n",
    "   information_schema.columns\n",
    "WHERE\n",
    "   table_name = 'usnewspaper';\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "result = conn.execute(sqlquery)\n",
    "\n",
    "data = [i for i in result]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = text(\n",
    "    \"\"\"SELECT DISTINCT title, news, keywords, url, publishdate, src \n",
    "    FROM usnewspaper \n",
    "    WHERE ARRAY['cte','lawsuit']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords, url, publishdate, src  \n",
    "     FROM usnewspaper \n",
    "     WHERE ARRAY['nfl', 'helmet']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords, url, publishdate, src  \n",
    "     FROM usnewspaper \n",
    "     WHERE ARRAY['nfl', 'brain']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords, url, publishdate, src \n",
    "    FROM usnewspaper \n",
    "    WHERE ARRAY['encephalopathy']::text[] <@ keywords AND news is not null;\"\"\"\n",
    ")\n",
    "result = conn.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39737a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(\n",
    "    data, columns=[\"title\", \"news\", \"keywords\", \"url\", \"publishdate\", \"src\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb76390",
   "metadata": {},
   "source": [
    "## Perform Named Entity Recognition (NER) Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp_spacy.pipe(df[\"news\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ents = []\n",
    "for doc in docs:\n",
    "    list_of_ents.append(\n",
    "        list(\n",
    "            set(\n",
    "                [\n",
    "                    ent.text\n",
    "                    for ent in doc.ents\n",
    "                    if (ent.label_ == \"ORG\") or (ent.label_ == \"PERSON\")\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164692e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"named_entities\"] = list_of_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c74bb6",
   "metadata": {},
   "source": [
    "## Now Perform LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.news.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub(r\"\\S*@\\S*\\s?\", \"\", sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub(r\"\\s+\", \" \", sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"'\", \"\", sent) for sent in data]\n",
    "data = [re.sub(\"`\", \"\", sent) for sent in data]\n",
    "data = [re.sub(\"´\", \"\", sent) for sent in data]\n",
    "\n",
    "# print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c747be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (\n",
    "            gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "        )  # deacc=True removes punctuations\n",
    "\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "# print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bb95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def bigrams_and_trigrams(texts):\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 2 times or more).\n",
    "    bigram = gensim.models.Phrases(texts, min_count=2)\n",
    "    for idx in range(len(texts)):\n",
    "        for token in bigram[texts[idx]]:\n",
    "            if \"_\" in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                texts[idx].append(token)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(\n",
    "    data_words_nostops, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    ")\n",
    "\n",
    "data_bigrams = bigrams_and_trigrams(data_lemmatized)\n",
    "data_trigrams = bigrams_and_trigrams(data_bigrams)\n",
    "# print(data_lemmatized[0])\n",
    "# print(data_bigrams[0])\n",
    "# print(data_trigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98114518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Frequency list\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "# print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=4,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha=\"auto\",\n",
    "    per_word_topics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57be2b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# personal_conduct_words = [\n",
    "#     id2word[term[0]] for term in lda_model.get_topic_terms(0, topn=20)\n",
    "# ]\n",
    "# personal_conduct_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7944136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print(\"\\nPerplexity: \", lda_model.log_perplexity(corpus))\n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence=\"c_v\"\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(\"\\nCoherence Score: \", coherence_lda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bdd38cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_nes_exploded = df.explode(\"named_entities\")\n",
    "\n",
    "df_names_matched = df_nes_exploded[\n",
    "    df_nes_exploded[\"named_entities\"]\n",
    "    .str.lower()\n",
    "    .isin(ab_fullname_cand[\"ltable_Clean_Name\"])\n",
    "]\n",
    "df_names_matched = df_names_matched.loc[\n",
    "    df_names_matched[\"title\"].drop_duplicates().index\n",
    "]\n",
    "df_names_matched"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03efb6d-89d1-4053-8afc-bee98f1b2fe9",
   "metadata": {},
   "source": [
    "### Construct Neo4j Node CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55510d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNewsNodes(data, node_file):\n",
    "    nodes = {}\n",
    "    counter = 732\n",
    "    node_header = [\"newsId:ID\", \"Title\", \"Publish Date\", \"Source\", \"URL\", \":LABEL\"]\n",
    "\n",
    "    # Construct node map:\n",
    "    for index, row in data.iterrows():\n",
    "\n",
    "        counter += 1\n",
    "        nodes[counter] = [\n",
    "            row[\"title\"],\n",
    "            row[\"publishdate\"],\n",
    "            row[\"src\"],\n",
    "            row[\"url\"].strip(),\n",
    "            f\"{row['src'].split('.')[1]}_{row['publishdate']}\",\n",
    "        ]\n",
    "\n",
    "    # write nodes CSV file\n",
    "    with open(node_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(node_header)\n",
    "        for node in nodes:\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    node,\n",
    "                    nodes[node][0],\n",
    "                    nodes[node][1],\n",
    "                    nodes[node][2],\n",
    "                    nodes[node][3],\n",
    "                    nodes[node][4],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    return nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783e0a8-cd7f-42f9-9609-f434486b6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNodes(data, node_file):\n",
    "    nodes = {}\n",
    "    counter = 1\n",
    "    node_header = [\n",
    "        \"playerId:ID\",\n",
    "        \"Name\",\n",
    "        \"PlayerID\",\n",
    "        \"Age\",\n",
    "        \"Birthday\",\n",
    "        \"Status\",\n",
    "        \"College\",\n",
    "        \":LABEL\",\n",
    "    ]\n",
    "\n",
    "    # Set start time to calculate compute time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Construct node map:\n",
    "    for index, row in data.iterrows():\n",
    "        parent_node_id = row.parent\n",
    "        child_node_id = row[\"ltable_Player Id\"]\n",
    "\n",
    "        if parent_node_id is None or child_node_id is None:\n",
    "            continue\n",
    "\n",
    "        # Check if parent node already mapped, otherwise add\n",
    "        if not bool([i for i in nodes if nodes[i][0] == parent_node_id]):\n",
    "            nodes[counter] = [\n",
    "                parent_node_id,\n",
    "                parent_node_id,\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                \"\",\n",
    "                parent_node_id,\n",
    "            ]\n",
    "            counter += 1\n",
    "\n",
    "        # Check if child node already mapped, otherwise add\n",
    "        if not bool([i for i in nodes if nodes[i][0] == child_node_id]):\n",
    "            nodes[counter] = [\n",
    "                row[\"ltable_Clean_Name\"] if child_node_id != \"NFL\" else \"\",\n",
    "                child_node_id if child_node_id != \"NFL\" else \"\",\n",
    "                row[\"ltable_Age\"]\n",
    "                if (child_node_id != \"NFL\" and row[\"ltable_Age\"] == row[\"ltable_Age\"])\n",
    "                else \"\",\n",
    "                row[\"ltable_Birthday\"]\n",
    "                if (\n",
    "                    child_node_id != \"NFL\"\n",
    "                    and row[\"ltable_Birthday\"] == row[\"ltable_Birthday\"]\n",
    "                )\n",
    "                else \"\",\n",
    "                row[\"ltable_Current Status\"] if child_node_id != \"NFL\" else \"\",\n",
    "                row[\"ltable_College\"] if child_node_id != \"NFL\" else \"\",\n",
    "                row[\"ltable_Clean_Name\"] if child_node_id != \"NFL\" else \"\",\n",
    "            ]\n",
    "            counter += 1\n",
    "\n",
    "    # write nodes CSV file\n",
    "    with open(node_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(node_header)\n",
    "        for node in nodes:\n",
    "            if nodes[node][0] == \"NFL\":\n",
    "                writer.writerow(\n",
    "                    [\n",
    "                        node,\n",
    "                        nodes[node][0],\n",
    "                        nodes[node][0],\n",
    "                        \"\",\n",
    "                        \"\",\n",
    "                        \"\",\n",
    "                        \"\",\n",
    "                        nodes[node][6],\n",
    "                    ]\n",
    "                )\n",
    "            else:\n",
    "                writer.writerow(\n",
    "                    [\n",
    "                        node,\n",
    "                        nodes[node][0],\n",
    "                        nodes[node][1],\n",
    "                        nodes[node][2],\n",
    "                        nodes[node][3],\n",
    "                        nodes[node][4],\n",
    "                        nodes[node][5],\n",
    "                        nodes[node][6],\n",
    "                    ]\n",
    "                )\n",
    "\n",
    "    # compute execution time\n",
    "    exec_time = time.time() - start_time\n",
    "\n",
    "    return nodes, exec_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c6d2d-0882-45d8-beb8-2207d40f73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data dump path for Neo4j\n",
    "# neo4j_data_path = \"/Users/camm/Library/NEO4J_HOME/import\"\n",
    "neo4j_data_path = \"/Users/galore/Downloads/neo4j-community-4.4.14/import/\"\n",
    "\n",
    "# Construct Node CSV file\n",
    "news_node_map = processNewsNodes(\n",
    "    df_names_matched, f\"{neo4j_data_path}/CTE_NEWS_Nodes.csv\"\n",
    ")\n",
    "node_map, exec_time = processNodes(\n",
    "    ab_fullname_cand.copy(), neo4j_data_path + \"/CTE_Nodes.csv\"\n",
    ")\n",
    "print(\"Exec time --- %s seconds ---\" % exec_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db6c1768",
   "metadata": {},
   "outputs": [],
   "source": [
    "[player_id for player_id, val in node_map.items() if val[0] == \"clarence verdin\"]\n",
    "# [player_id for player_id, val in node_map.items() if val[0] == \"bill larson\"]\n",
    "# [val for player_id, val in node_map.items()]\n",
    "# news_node_map\n",
    "len(node_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc6119-c05e-4185-967a-77d472bebb86",
   "metadata": {},
   "source": [
    "### Construct Neo4j Relations CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd77644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNewsRelations(news_data, player_nodes, rel_file):\n",
    "    relation_header = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
    "    relation_data = []\n",
    "    counter = 733\n",
    "\n",
    "    # Construct relation map:\n",
    "\n",
    "    for index, row in news_data.iterrows():\n",
    "        player_id = [\n",
    "            p_id\n",
    "            for p_id, val in player_nodes.items()\n",
    "            if val[0] == row[\"named_entities\"].lower()\n",
    "        ]\n",
    "        if len(player_id) == 1:\n",
    "            relation_data.append([player_id[0], counter, \"MENTIONED_IN\"])\n",
    "        counter += 1\n",
    "\n",
    "    # wirte relation file\n",
    "    with open(rel_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(relation_header)\n",
    "        for row in relation_data:\n",
    "            writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8834882-7aa1-4c1d-b1d9-8ab95a237daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRelations(data, nodes, rel_file):\n",
    "    relation_header = [\":START_ID\", \":END_ID\", \":TYPE\"]\n",
    "    relation_data = []\n",
    "\n",
    "    # Set start time to calculate compute time\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Construct relation map:\n",
    "    for index, row in data.iterrows():\n",
    "        if row.direction == \"parent_to_child\":\n",
    "            # relation_data.append([nodes[list(nodes.keys()) [list(nodes.values()).index(row.parent)]],\n",
    "            #                       nodes[list(nodes.keys()) [list(nodes.values()).index(row['ltable_Player Id'])]],\n",
    "            #                       row['rtable_cte_category']])\n",
    "\n",
    "            relation_data.append(\n",
    "                [\n",
    "                    [i for i in nodes if nodes[i][1] == row.parent][0],\n",
    "                    [i for i in nodes if nodes[i][1] == row[\"ltable_Player Id\"]][0],\n",
    "                    row[\"rtable_cte_category\"],\n",
    "                ]\n",
    "            )\n",
    "        else:\n",
    "            # relation_data.append([nodes[list(node_map.keys()) [list(nodes.values()).index(row['ltable_Player Id'])]],\n",
    "            #                       nodes[list(node_map.keys()) [list(nodes.values()).index(row.parent)]],\n",
    "            #                       row['rtable_cte_category']])\n",
    "\n",
    "            relation_data.append(\n",
    "                [\n",
    "                    [i for i in nodes if nodes[i][0] == row[\"ltable_Player Id\"]][0],\n",
    "                    [i for i in nodes if nodes[i][0] == row.parent][0],\n",
    "                    row[\"rtable_cte_category\"],\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    # wirte relation file\n",
    "    with open(rel_file, \"w\", newline=\"\") as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(relation_header)\n",
    "        writer.writerows(relation_data)\n",
    "\n",
    "    # compute execution time\n",
    "    exec_time = time.time() - start_time\n",
    "\n",
    "    return exec_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b9c88-b464-455c-854c-99d7af57edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct relation 'cites' & 'family-cites' CSV file\n",
    "exec_time = processRelations(\n",
    "    ab_fullname_cand.copy(), node_map, neo4j_data_path + \"/CTE_Relations.csv\"\n",
    ")\n",
    "processNewsRelations(\n",
    "    df_names_matched, node_map, f\"{neo4j_data_path}/CTE_NEWS_Relations.csv\"\n",
    ")\n",
    "print(\"Exec time --- %s seconds ---\" % exec_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc747588-7cc3-4eff-acb5-f0f03a45fb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example query on enriched knowledge graph:\n",
    "# MATCH p=()-[r:MENTIONED_IN|affected_players]->() RETURN p LIMIT 25\n",
    "# MATCH p=()-[r:MENTIONED_IN|suspected_deceased_players]->() RETURN p LIMIT 25\n",
    "# MATCH p=()-[r:cte_als_former_players|MENTIONED_IN ]->() RETURN p LIMIT 25\n",
    "# MATCH p=()-[r:suspected_deceased_players|MENTIONED_IN ]->() RETURN p LIMIT 25\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293aec93-c9ba-418c-a51f-2fd7e9801cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5668744c-3a85-48e8-b4de-e8d323d5a6ae",
   "metadata": {},
   "source": [
    "## Text to Knowledge Graph - Demo Example for Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24799202-ff7c-485b-9c01-bfd37012e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy, psycopg2\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "from spacy.matcher import Matcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "%matplotlib inline\n",
    "\n",
    "sentences = nltk.sent_tokenize(wiki_page_object.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654190b-3c16-4499-a2b1-557c647cefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Change below creds/config per your own DB settings:\n",
    "# ----------------------------------------------------\n",
    "db_host = \"awesome-hw.sdsc.edu\"  # <- enter your DB host name\n",
    "db_name = \"postgres\"  # <- enter your DB name\n",
    "db_username = \"ag_class\"  # <- enter your DB username\n",
    "db_password = \"WUcgdfQ1\"  # <- enter your DB password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec1987-fbf5-4dfe-99a3-9d351927906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a postgresql engine instance\n",
    "print(\n",
    "    \"Connection string: postgresql://\"\n",
    "    + db_username\n",
    "    + \":\"\n",
    "    + db_password\n",
    "    + \"@\"\n",
    "    + db_host\n",
    "    + \"/\"\n",
    "    + db_name\n",
    ")\n",
    "alchemyEngine = sqlalchemy.create_engine(\n",
    "    \"postgresql://\" + db_username + \":\" + db_password + \"@\" + db_host + \"/\" + db_name\n",
    ")\n",
    "%reload_ext sql\n",
    "%sql $alchemyEngine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc21d3-ede1-4d89-8b23-c0f0de01bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to PostgreSQL server\n",
    "dbConnection = alchemyEngine.connect()\n",
    "\n",
    "# SQL command\n",
    "sql = \"\"\"\n",
    "        SELECT DISTINCT title, news, keywords from usnewspaper WHERE ARRAY['cte','lawsuit']::text[] <@ keywords AND news IS NOT NULL\n",
    "        UNION\n",
    "        /*\n",
    "        SELECT DISTINCT title, news, keywords  from usnewspaper WHERE ARRAY['nfl', 'helmet']::text[] <@ keywords AND news IS NOT NULL\n",
    "        UNION\n",
    "        SELECT DISTINCT title, news, keywords  from usnewspaper WHERE ARRAY['nfl', 'brain']::text[] <@ keywords AND news IS NOT NULL\n",
    "        UNION */\n",
    "        SELECT DISTINCT title, news, keywords from usnewspaper WHERE ARRAY['encephalopathy']::text[] <@ keywords \n",
    "             AND news IS NOT NULL AND title ilike '%nfl%';\n",
    "      \"\"\"\n",
    "\n",
    "# Read data from PostgreSQL database table and load into a DataFrame instance\n",
    "news_data_df = pd.read_sql_query(sqlalchemy.text(sql), alchemyEngine)\n",
    "\n",
    "# Close the database connection\n",
    "dbConnection.close()\n",
    "\n",
    "pd.set_option(\"display.expand_frame_repr\", False)\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "# Display the DataFrame\n",
    "display(news_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3e5fa-c5bf-4890-b856-27d6cc936e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEntities(sent):\n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"  # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"  # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "    #############################################################\n",
    "\n",
    "    for tok in nlp(sent):\n",
    "        ## chunk 2\n",
    "        # # if token is a punctuation mark then move on to the next token\n",
    "        # if tok.dep_ != \"punct\":\n",
    "\n",
    "        # check: token is a compound word or not\n",
    "        if tok.dep_ == \"compound\":\n",
    "            prefix = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                prefix = prv_tok_text + \" \" + tok.text\n",
    "\n",
    "        # check: token is a modifier or not\n",
    "        if tok.dep_.endswith(\"mod\") == True:\n",
    "            modifier = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                modifier = prv_tok_text + \" \" + tok.text\n",
    "\n",
    "        ## chunk 3\n",
    "        if tok.dep_.find(\"subj\") == True:\n",
    "            ent1 = modifier + \" \" + prefix + \" \" + tok.text\n",
    "            prefix = \"\"\n",
    "            modifier = \"\"\n",
    "            prv_tok_dep = \"\"\n",
    "            prv_tok_text = \"\"\n",
    "\n",
    "        ## chunk 4\n",
    "        if tok.dep_.find(\"obj\") == True:\n",
    "            ent2 = modifier + \" \" + prefix + \" \" + tok.text\n",
    "\n",
    "        ## chunk 5\n",
    "        # update variables\n",
    "        prv_tok_dep = tok.dep_\n",
    "        prv_tok_text = tok.text\n",
    "    #############################################################\n",
    "\n",
    "    return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "\n",
    "def GetRelation(sent):\n",
    "\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    # define the pattern\n",
    "    pattern = [\n",
    "        {\"DEP\": \"ROOT\"},\n",
    "        {\"DEP\": \"prep\", \"OP\": \"?\"},\n",
    "        {\"DEP\": \"agent\", \"OP\": \"?\"},\n",
    "        {\"POS\": \"ADJ\", \"OP\": \"?\"},\n",
    "    ]\n",
    "\n",
    "    matcher.add(\"matching_1\", [pattern], greedy=\"LONGEST\")\n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "\n",
    "    span = doc[matches[k][1] : matches[k][2]]\n",
    "\n",
    "    return span.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2b249-deab-4dfc-a0ef-b19fef12cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(\n",
    "    RemoveStopWords(\n",
    "        re.sub(\n",
    "            \"\\n|\\r\",\n",
    "            \" \",\n",
    "            news_data_df[\"news\"]\n",
    "            .replace(r\"[^\\W\\s]+(?<!.)\", \"\", regex=True)[1]\n",
    "            .replace(\"“\", \"\")\n",
    "            .replace(\"”\", \"\")\n",
    "            .replace(\"’\", \"\")\n",
    "            .replace(\"'\", \"\")\n",
    "            .lower(),\n",
    "        )\n",
    "    )\n",
    ")\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389e2d1-2e09-431e-b399-a47c776a214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(sentences)):\n",
    "    if (\n",
    "        GetEntities(str(sentences[i]))[0] != \"\"\n",
    "        and GetEntities(str(sentences[i]))[1] != \"\"\n",
    "    ):\n",
    "        print(\n",
    "            f\"Ents: {GetEntities(str(sentences[i]))} \\t\\t\\tRels: {GetRelation(str(sentences[i]))}\"\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
