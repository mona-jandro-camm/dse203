{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e3fc0a7-3976-4659-aba4-4b2b4863ad35",
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------------------\n",
    "#   Author Name:        Camm Perera, Ramona Henry,  Alejandro Hohmann          \n",
    "#   Create Date:        12-06-2022\n",
    "#   Description:        DSE-203 - Group #5, NFL-CTE Knowledge Graph\n",
    "#   System specs:       \n",
    "#        MacOS Monterey   : 12.5.1 \n",
    "#        Python           : 3.8.13 \n",
    "#        IPython          : 8.4.0\n",
    "#        ipykernel        : 6.15.2\n",
    "#        ipywidgets       : 7.6.5\n",
    "#        jupyter_client   : 6.1.12\n",
    "#        jupyter_core     : 4.10.0\n",
    "#        jupyter_server   : 1.18.1\n",
    "#        jupyterlab       : 3.4.4\n",
    "#        nbclient         : 0.5.13\n",
    "#        nbconvert        : 6.4.4\n",
    "#        nbformat         : 5.5.0\n",
    "#        notebook         : 6.4.12\n",
    "#        qtconsole        : 5.3.2\n",
    "#        traitlets        : 5.1.1\n",
    "# #---------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5341ac0f-55b8-424d-a30f-a4434ac2ff3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import py_stringmatching as sm \n",
    "import py_entitymatching as em\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import re, string, math, time\n",
    "import wikipedia\n",
    "import stanza\n",
    "import requests\n",
    "import csv\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "\n",
    "import sqlalchemy as sal\n",
    "from sqlalchemy import text\n",
    "\n",
    "import nltk\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim_models as gensimvis\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import spacy\n",
    "\n",
    "#nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a63ae-9c86-46fb-bf8c-4518ff397863",
   "metadata": {},
   "source": [
    "### Load  Kaggle Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7fb67-cbe3-45c4-a922-07de881bd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_stats_df = em.read_csv_metadata(\"../../Datasets/Basic_Stats.csv\" ,key='Player Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e986dbf-3de9-4744-a59e-5f7a4dbd39e2",
   "metadata": {},
   "source": [
    "### Extract Wikipedia Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42a1464c-bb65-44c5-92b5-92885a10f579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "wiki_title = 'List of NFL players with chronic traumatic encephalopathy'\n",
    "wiki_url = 'https://en.wikipedia.org/wiki/List_of_NFL_players_with_chronic_traumatic_encephalopathy'\n",
    "\n",
    "# Python Wikipedia library\n",
    "wiki_page_object     = wikipedia.page(wiki_title)\n",
    "\n",
    "# Python Beautiful Soup\n",
    "wiki_page = requests.get(wiki_url)\n",
    "soup = BeautifulSoup(wiki_page.content, \"lxml\")\n",
    "\n",
    "# print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01def4c0-d489-41c4-b2e9-d7d7ef27d4fb",
   "metadata": {},
   "source": [
    "#### Stanza - stanford NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f82d67-065a-49d6-b076-527f3b388d5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline('en', processors='tokenize,mwt,ner', use_gpu=False, pos_batch_size=3000, download_method=None)  # This sets up a default neural pipeline in English"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b18b5d-7aa5-43c7-a7d4-82781860b89c",
   "metadata": {},
   "source": [
    "#### Process Players Affected Wiki Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c9f861-e9ad-4a36-8b06-84265ad99fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "players_affected_ls = []\n",
    "\n",
    "# Wiki-Extract Players affected\n",
    "players_affected_ls = soup.select('p')[4:8] \n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# Create lists to hold Person lists\n",
    "affected_players_ls = []\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "for index in players_affected_ls:\n",
    "    doc = nlp(str(index))                                 \n",
    "\n",
    "    # Extract PERSON & ORG entities\n",
    "    for ent in doc.ents:\n",
    "        if (ent.type =='PERSON'):\n",
    "            clean_name = re.split('</a', ent.text)[0] \n",
    "            affected_players_ls.append(clean_name)\n",
    "            \n",
    "# Dedupe list contents\n",
    "affected_players_ls = [*set(affected_players_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time)) \n",
    "print(f'# of person: {len(affected_players_ls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf6315cc-df6f-489d-a165-0d87b5277ee7",
   "metadata": {},
   "source": [
    "#### Process Former Players affected with CTE Wiki Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77086550-6b7e-43e9-9176-e9aa8b2d5789",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "former_players_post_mortem_ls = []\n",
    "pm_former_players_ls = []\n",
    "\n",
    "# Wiki-Extract Former players with CTE confirmed post-mortem\n",
    "results = soup.select('ul')[1]\n",
    "former_players_post_mortem_ls = results.find_all(\"a\")\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "for index in former_players_post_mortem_ls:\n",
    "    doc = nlp(str(index))                                 \n",
    "\n",
    "    # Extract PERSON entities\n",
    "    for ent in doc.ents:\n",
    "        if (ent.type =='PERSON'):\n",
    "            clean_name = re.split('</a', ent.text)[0] \n",
    "            pm_former_players_ls.append(clean_name)\n",
    "            \n",
    "# Dedupe list contents\n",
    "pm_former_players_ls = [*set(pm_former_players_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time)) \n",
    "print(f'# of person: {len(pm_former_players_ls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78e7a807-ba48-4bf9-8a3d-5d5174890250",
   "metadata": {},
   "source": [
    "#### Process Deceased players suspected of having had CTE Wiki Sction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19d5e522-7f5f-42f0-a37d-e722d02368b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "deceased_players_ls = []\n",
    "suspected_deceased_players_ls = []\n",
    "\n",
    "# Wiki-Extract Former players with CTE confirmed post-mortem\n",
    "results = soup.select('ul')[2]\n",
    "deceased_players_ls = results.find_all(\"a\")\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "for index in deceased_players_ls:\n",
    "    doc = nlp(str(index))                                 \n",
    "\n",
    "    # Extract PERSON entities\n",
    "    for ent in doc.ents:\n",
    "        if (ent.type =='PERSON'):\n",
    "            clean_name = re.split('</a', ent.text)[0] \n",
    "            suspected_deceased_players_ls.append(clean_name)\n",
    "            \n",
    "# Dedupe list contents\n",
    "suspected_deceased_players_ls = [*set(suspected_deceased_players_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time)) \n",
    "print(f'# of person: {len(suspected_deceased_players_ls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94465ac-5964-4415-b9a2-fbf7442cf5c2",
   "metadata": {},
   "source": [
    "#### Process Living former players diagnosed with CTE or ALS or reporting symptoms consistent with CTE or ALS Wiki Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44e7fe-5d4a-4aa7-a8a0-41b5f86b6aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "former_cte_als_players_ls = []\n",
    "cte_als_former_players_ls = []\n",
    "\n",
    "# Wiki-Extract Former players with CTE confirmed post-mortem\n",
    "results = soup.select('ul')[3]\n",
    "former_cte_als_players_ls = results.find_all(\"a\")\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "for index in former_cte_als_players_ls:\n",
    "    doc = nlp(str(index))                                 \n",
    "\n",
    "    # Extract PERSON entities\n",
    "    for ent in doc.ents:\n",
    "        if (ent.type =='PERSON'):\n",
    "            clean_name = re.split('</a', ent.text)[0] \n",
    "            cte_als_former_players_ls.append(clean_name)\n",
    "            \n",
    "# Dedupe list contents\n",
    "cte_als_former_players_ls = [*set(cte_als_former_players_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time)) \n",
    "print(f'# of person: {len(cte_als_former_players_ls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8dbeae-dcb7-42e7-ab30-86e561572fb0",
   "metadata": {},
   "source": [
    "#### Process Former players listed as plaintiffs in lawsuits against the NFL for concussion-related injuries received after Wiki playing Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "993325cd-c0d9-4fef-aea9-22e509a7bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store player names by category\n",
    "players_lawsuits_nfl_ls = []\n",
    "players_nfl_lawsuits_ls = []\n",
    "\n",
    "# Wiki-Extract Former players with CTE confirmed post-mortem\n",
    "results = soup.select('ul')[4]\n",
    "players_lawsuits_nfl_ls = results.find_all(\"a\")\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "\n",
    "# When there are many texts, creating all of the stanza docs at once is faster\n",
    "docs_in = [stanza.Document([], text=str(d)) for d in players_lawsuits_nfl_ls]\n",
    "docs_out = nlp(docs_in)\n",
    "\n",
    "for doc in docs_out:\n",
    "    # Extract PERSON & ORG entities\n",
    "    for ent in doc.ents:\n",
    "        if (ent.type =='PERSON'):\n",
    "            clean_name = re.split('</a', ent.text)[0] \n",
    "            players_nfl_lawsuits_ls.append(clean_name)\n",
    "\n",
    "# for doc in docs_out:\n",
    "#     # Extract PERSON & ORG entities\n",
    "#     for ent in doc.ents:\n",
    "#         if (ent.type =='PERSON'):\n",
    "#             clean_name = re.split('', ent.text)[0] \n",
    "#             players_nfl_lawsuits_ls.append(clean_name)\n",
    "\n",
    "# for index in players_lawsuits_nfl_ls:\n",
    "#     doc = nlp(str(index))                                 \n",
    "# \n",
    "#     # Extract PERSON entities\n",
    "#     for ent in doc.ents:\n",
    "#         if (ent.type =='PERSON'):\n",
    "#             clean_name = re.split('</a', ent.text)[0] \n",
    "#             players_nfl_lawsuits_ls.append(clean_name)\n",
    "#             \n",
    "# Dedupe list contents\n",
    "players_nfl_lawsuits_ls = [*set(players_nfl_lawsuits_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time)) \n",
    "print(f'# of person: {len(players_nfl_lawsuits_ls)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e7386e-f4d0-4392-a9b6-b7a79f1b06c4",
   "metadata": {},
   "source": [
    "### Text Normalization & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3297875-3f0a-4671-818b-f4136b3393d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Normalize player \"Name\" in Kaggle basic stats\n",
    "# ---------------------------------------------------\n",
    "basic_stats_df['Clean_Name'] = basic_stats_df.Name.str.lower().map(lambda s: s.split()[1] + ' ' + s.split()[0]).replace('[^\\w\\s]',' ', regex=True).str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0229216f-eaa1-4339-ba80-c50ccb7976ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------\n",
    "# Remove punctuations & lower name \n",
    "# ---------------------------------------------------\n",
    "def remove_punc(name):\n",
    "    punc = '''!()-[]{};:'\"\\, <>./?@#$%^&*_~'''\n",
    "    for ele in name:  \n",
    "        if ele in punc:  \n",
    "            names = name.replace(ele, \" \") \n",
    "    return name.lower().strip()\n",
    "\n",
    "affected_players_ls  = [remove_punc(i) for i in affected_players_ls]\n",
    "pm_former_players_ls = [remove_punc(i) for i in pm_former_players_ls]\n",
    "suspected_deceased_players_ls  = [remove_punc(i) for i in suspected_deceased_players_ls]\n",
    "cte_als_former_players_ls  = [remove_punc(i) for i in cte_als_former_players_ls]\n",
    "players_nfl_lawsuits_ls  = [remove_punc(i) for i in players_nfl_lawsuits_ls]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0023ce85-64c9-4e13-8ee4-8f3c4ecdfbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------\n",
    "# Create combo dataframe for each list(above) category \n",
    "# --------------------------------------------------------\n",
    "affected_players_df = pd.DataFrame(data=[['affected_players'] * len(affected_players_ls), affected_players_ls]).T\n",
    "affected_players_df.columns = ['cte_category', 'Clean_Name']\n",
    "\n",
    "pm_former_players_df = pd.DataFrame(data=[['pm_former_players'] * len(pm_former_players_ls), pm_former_players_ls]).T\n",
    "pm_former_players_df.columns = ['cte_category', 'Clean_Name']\n",
    "\n",
    "suspected_deceased_players_df = pd.DataFrame(data=[['suspected_deceased_players'] * len(suspected_deceased_players_ls), suspected_deceased_players_ls]).T\n",
    "suspected_deceased_players_df.columns = ['cte_category', 'Clean_Name']\n",
    "\n",
    "cte_als_former_players_df = pd.DataFrame(data=[['cte_als_former_players'] * len(cte_als_former_players_ls), cte_als_former_players_ls]).T\n",
    "cte_als_former_players_df.columns = ['cte_category', 'Clean_Name']\n",
    "\n",
    "players_nfl_lawsuits_df = pd.DataFrame(data=[['players_nfl_lawsuits'] * len(players_nfl_lawsuits_ls), players_nfl_lawsuits_ls]).T\n",
    "players_nfl_lawsuits_df.columns = ['cte_category', 'Clean_Name']\n",
    "\n",
    "# Combine dataframes\n",
    "frames = [affected_players_df, pm_former_players_df, suspected_deceased_players_df, cte_als_former_players_df, players_nfl_lawsuits_df]\n",
    "wiki_cte_players_df = pd.concat(frames)\n",
    "wiki_cte_players_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4c7c76-1b12-43fe-9f3c-c773c0af1489",
   "metadata": {},
   "source": [
    "#### Create CSV file and em.DataFrame for Entity Matching "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1ea03b-a3bf-432a-a0fc-ceb5454b32e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create CSV & entity match dataframe for blocking\n",
    "wiki_cte_players_df['rec_id'] = range(1, 1+len(wiki_cte_players_df))\n",
    "wiki_cte_players_df.to_csv(\"./wiki_cte_players_df.csv\")\n",
    "wiki_person_df = em.read_csv_metadata(\"./wiki_cte_players_df.csv\", key='rec_id')\n",
    "wiki_person_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d1f507-b5ce-499a-9f54-afb164425da7",
   "metadata": {},
   "source": [
    "#### Block DataFrames to get Candidate set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d07a0920-95f1-4662-a925-4f425ff7f6a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  Instantiate blocker objects:\n",
    "# ------------------------------\n",
    "# Create overlap blocker\n",
    "ob = em.OverlapBlocker()\n",
    "\n",
    "# Create attribute equivalence blocker\n",
    "ab = em.AttrEquivalenceBlocker()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc3eac2-0878-4ea3-9c92-74348c7664d7",
   "metadata": {},
   "source": [
    "#### i. Overlap Block by 'player_name'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aefd457-13e7-41d6-bc3a-5298648f136d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Block tables using full name\n",
    "ob_fullname_cand = ob.block_tables(basic_stats_df, wiki_person_df, 'Clean_Name', 'Clean_Name', allow_missing=False,\n",
    "                                l_output_attrs=['Player Id', 'Name',  'Age', 'Current Status', 'Birthday', 'College','High School', 'Clean_Name'],\n",
    "                                r_output_attrs=['rec_id', 'Clean_Name', 'cte_category'],\n",
    "                                overlap_size=2, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad3b4f4c-bdd9-43b2-b95e-dc836db26c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Matched candidates - Kaggle vs. Wiki page\n",
    "ob_fullname_cand.to_csv('./OB_names_matched.csv')\n",
    "ob_fullname_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78c77006-b8ec-46a9-a79d-3cfd163962c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug blocker output : (FOR TESTING ONLY)\n",
    "# #  Unmatched candidates - Kaggle vs. Wiki page\n",
    "# corres = [('Clean_Name', 'Clean_Name')]\n",
    "# ob_fullname_debug = em.debug_blocker(ob_fullname_cand, basic_stats_df, wiki_person_df, output_size=500, attr_corres=corres)\n",
    "\n",
    "# # Display first few tuple pairs from the debug_blocker's output\n",
    "# ob_fullname_debug  #.to_csv('./names_debug.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67ac10c-a0d2-4f05-adcf-51b8c295789d",
   "metadata": {},
   "source": [
    "#### ii. Attribute Block by 'player_name'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff23fd-66e9-4b4b-ac70-430c1148f2b2",
   "metadata": {},
   "source": [
    "### <font color='red'> *** BETTER RESULTS THAN OVERLAP BLOCK ***</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1581e6d3-9993-467a-88e7-21309bddb3c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block using 'full_name_dob' attribute\n",
    "ab_fullname_cand = ab.block_tables(basic_stats_df, wiki_person_df, 'Clean_Name', 'Clean_Name', allow_missing=False,\n",
    "                                l_output_attrs=['Player Id', 'Name',  'Age', 'Current Status', 'Birthday', 'College','High School', 'Clean_Name'],\n",
    "                                r_output_attrs=['rec_id', 'Clean_Name', 'cte_category'], n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fcae0b-b8ea-4b08-9b9e-6976b73b8189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Distinct matched candidates - Kaggle vs. Wiki page\n",
    "ab_fullname_cand.groupby(\"ltable_Player Id\").first().to_csv('./AB_names_matched.csv')\n",
    "ab_fullname_cand.groupby(\"ltable_Player Id\").first()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ae542f-2980-4c8a-a1a0-6a3b5f9c02a1",
   "metadata": {},
   "source": [
    "#### Process Organizations from Wiki Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7ac3bc-6eb0-4dca-b5fc-08028b13c33a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove stop words\n",
    "def RemoveStopWords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "    filtered_sentence = []\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "             filtered_sentence.append(w)\n",
    "    return ' '.join(filtered_sentence)\n",
    "\n",
    "# Remove numbers\n",
    "def RemoveNumbers(text):\n",
    "    return re.sub(r'\\d+', '', text) \n",
    "\n",
    "# Remove Punctuations\n",
    "def RemovePunctuations(text):\n",
    "    # return re.sub(rf\"[{string.punctuation}]\", \" \", text)\n",
    "    return re.sub(r'[^\\w\\s]', ' ', text)\n",
    "    \n",
    "# Normalize text \n",
    "def NormalizeText(text):\n",
    "    result = text\n",
    "    # result = RemoveNumbers(result)      # Remove any numbers\n",
    "    result = RemovePunctuations(result) # Remove any punctuations\n",
    "    result = RemoveStopWords(result)    # Remove stop words\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2838a7-e6f9-430c-83c1-3e19732dc414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists to store wiki orgaization names\n",
    "wiki_org_ls = []\n",
    "\n",
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# PASS-1: Compute NER with wiki page links - Former players with CTE confirmed post-mortem\n",
    "doc = nlp(NormalizeText(wiki_page_object.content))                                 \n",
    "\n",
    "# Extract ORG entities\n",
    "for ent in doc.ents:\n",
    "    if (ent.type == 'ORG'):\n",
    "        wiki_org_ls.append(ent.text)\n",
    "            \n",
    "# Dedupe list contents\n",
    "wiki_org_ls = [*set(wiki_org_ls)]\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time)) \n",
    "print(f'# of organizations: {len(wiki_org_ls)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "232cee37-e91b-4f4b-8d4f-09a1f18af807",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display Wiki page Organizations\n",
    "wiki_org_df = pd.DataFrame(data= wiki_org_ls, columns=['wiki_org_name'])\n",
    "wiki_org_df['ord_id'] = wiki_org_df.index+1\n",
    "wiki_org_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9a5695-11aa-4d4e-8e5b-491d5488971d",
   "metadata": {},
   "source": [
    "#### Assign Parent Node & Direction to Players DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd18a32c-45e2-4e53-9b6d-5fcfaf8d7e65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_fullname_cand['parent']     =[wiki_org_df.query(\"wiki_org_name == 'NFL'\")['wiki_org_name'].values[0]] * len(ab_fullname_cand)\n",
    "ab_fullname_cand['direction']  =['parent_to_child'] * len(ab_fullname_cand)\n",
    "ab_fullname_cand['ltable_Age'] = pd.to_numeric(ab_fullname_cand.ltable_Age, downcast='integer')\n",
    "ab_fullname_cand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db985d6d-2b6b-491f-993b-329b2b7577ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "ab_fullname_cand.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b09d682f",
   "metadata": {},
   "source": [
    "## Newspaper Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3d523d",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = sal.create_engine(\n",
    "    \"postgresql+psycopg2://ag_class:WUcgdfQ1@awesome-hw.sdsc.edu/postgres\"\n",
    ")\n",
    "conn = engine.connect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242c06ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Schema query\n",
    "sqlquery = text(\n",
    "    \"\"\"\n",
    "SELECT\n",
    "   table_name,\n",
    "   column_name,\n",
    "   data_type\n",
    "FROM\n",
    "   information_schema.columns\n",
    "WHERE\n",
    "   table_name = 'usnewspaper';\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "result = conn.execute(sqlquery)\n",
    "\n",
    "data = [i for i in result]\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe77de",
   "metadata": {},
   "outputs": [],
   "source": [
    "sql_query = text(\n",
    "\"\"\"SELECT DISTINCT title, news, keywords \n",
    "    FROM usnewspaper \n",
    "    WHERE ARRAY['cte','lawsuit']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords  \n",
    "     FROM usnewspaper \n",
    "     WHERE ARRAY['nfl', 'helmet']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords  \n",
    "     FROM usnewspaper \n",
    "     WHERE ARRAY['nfl', 'brain']::text[] <@ keywords and news is not null\n",
    "UNION\n",
    "SELECT DISTINCT title, news, keywords \n",
    "    FROM usnewspaper \n",
    "    WHERE ARRAY['encephalopathy']::text[] <@ keywords AND news is not null;\"\"\"\n",
    ")   \n",
    "result = conn.execute(sql_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad47504e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [i for i in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39737a90",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data, columns=[\"title\", \"news\", \"keywords\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb76390",
   "metadata": {},
   "source": [
    "## Perform Named Entity Recognition (NER) Using Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d03faee",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_spacy = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9803e13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(nlp_spacy.pipe(df[\"news\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50fc69f",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_ents = []\n",
    "for doc in docs:\n",
    "    list_of_ents.append(\n",
    "        list(set([ent.text for ent in doc.ents if (ent.label_ == \"ORG\") or (ent.label_ == \"PERSON\")]))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164692e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"named_entities\"] = list_of_ents"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c74bb6",
   "metadata": {},
   "source": [
    "## Now Perform LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24e390",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267eef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to list\n",
    "data = df.news.values.tolist()\n",
    "\n",
    "# Remove Emails\n",
    "data = [re.sub(r\"\\S*@\\S*\\s?\", \"\", sent) for sent in data]\n",
    "\n",
    "# Remove new line characters\n",
    "data = [re.sub(r\"\\s+\", \" \", sent) for sent in data]\n",
    "\n",
    "# Remove distracting single quotes\n",
    "data = [re.sub(\"'\", \"\", sent) for sent in data]\n",
    "data = [re.sub(\"`\", \"\", sent) for sent in data]\n",
    "data = [re.sub(\"´\", \"\", sent) for sent in data]\n",
    "\n",
    "print(data[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c747be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(sentences):\n",
    "    for sentence in sentences:\n",
    "        yield (\n",
    "            gensim.utils.simple_preprocess(str(sentence), deacc=True)\n",
    "        )  # deacc=True removes punctuations\n",
    "\n",
    "\n",
    "data_words = list(sent_to_words(data))\n",
    "\n",
    "print(data_words[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85bb95d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define functions for stopwords and lemmatization\n",
    "def remove_stopwords(texts):\n",
    "    return [[word for word in doc if word not in stop_words] for doc in texts]\n",
    "\n",
    "\n",
    "def lemmatization(texts, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]):\n",
    "    \"\"\"https://spacy.io/api/annotation\"\"\"\n",
    "    texts_out = []\n",
    "    for sent in texts:\n",
    "        doc = nlp(\" \".join(sent))\n",
    "        texts_out.append(\n",
    "            [token.lemma_ for token in doc if token.pos_ in allowed_postags]\n",
    "        )\n",
    "    return texts_out\n",
    "\n",
    "\n",
    "def bigrams_and_trigrams(texts):\n",
    "\n",
    "    # Add bigrams and trigrams to docs (only ones that appear 2 times or more).\n",
    "    bigram = gensim.models.Phrases(texts, min_count=2)\n",
    "    for idx in range(len(texts)):\n",
    "        for token in bigram[texts[idx]]:\n",
    "            if \"_\" in token:\n",
    "                # Token is a bigram, add to document.\n",
    "                texts[idx].append(token)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4a19a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove Stop Words\n",
    "data_words_nostops = remove_stopwords(data_words)\n",
    "\n",
    "# Initialize spacy 'en' model, keeping only tagger component (for efficiency)\n",
    "# python3 -m spacy download en\n",
    "nlp = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])\n",
    "\n",
    "# Do lemmatization keeping only noun, adj, vb, adv\n",
    "data_lemmatized = lemmatization(\n",
    "    data_words_nostops, allowed_postags=[\"NOUN\", \"ADJ\", \"VERB\", \"ADV\"]\n",
    ")\n",
    "\n",
    "data_bigrams = bigrams_and_trigrams(data_lemmatized)\n",
    "data_trigrams = bigrams_and_trigrams(data_bigrams)\n",
    "# print(data_lemmatized[0])\n",
    "# print(data_bigrams[0])\n",
    "print(data_trigrams[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98114518",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(data_lemmatized)\n",
    "\n",
    "# Create Corpus\n",
    "texts = data_lemmatized\n",
    "\n",
    "# Term Frequency list\n",
    "corpus = [id2word.doc2bow(text) for text in texts]\n",
    "\n",
    "# View\n",
    "print(corpus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae4ae1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build LDA model\n",
    "lda_model = gensim.models.ldamodel.LdaModel(\n",
    "    corpus=corpus,\n",
    "    id2word=id2word,\n",
    "    num_topics=4,\n",
    "    random_state=100,\n",
    "    update_every=1,\n",
    "    chunksize=100,\n",
    "    passes=10,\n",
    "    alpha=\"auto\",\n",
    "    per_word_topics=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ea6eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = gensimvis.prepare(lda_model, corpus, id2word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f5f129",
   "metadata": {},
   "outputs": [],
   "source": [
    "vis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7944136",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print(\"\\nPerplexity: \", lda_model.log_perplexity(corpus))\n",
    "# a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(\n",
    "    model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence=\"c_v\"\n",
    ")\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print(\"\\nCoherence Score: \", coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03efb6d-89d1-4053-8afc-bee98f1b2fe9",
   "metadata": {},
   "source": [
    "### Construct Neo4j Node CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4783e0a8-cd7f-42f9-9609-f434486b6599",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processNodes(data, node_file):\n",
    "    nodes = {}\n",
    "    counter = 1\n",
    "    node_header = [\":ID\", \"Name\", \"PlayerID\" ,\"Age\", \"Birthday\", \"Status\", \"College\", \":LABEL\"]\n",
    "            \n",
    "    # Set start time to calculate compute time\n",
    "    start_time = time.time()\n",
    "\n",
    "     # Construct node map:\n",
    "    for index, row in data.iterrows():           \n",
    "        parent_node_id  = row.parent\n",
    "        child_node_id   = row['ltable_Player Id']\n",
    "        \n",
    "        if parent_node_id is None or child_node_id is None:\n",
    "            continue;\n",
    "        \n",
    "        # Check if parent node already mapped, otherwise add \n",
    "        if not bool([i for i in nodes if nodes[i][0] == parent_node_id]):       \n",
    "            nodes[counter]   = [parent_node_id, parent_node_id,'','','','', parent_node_id]\n",
    "            counter+=1       \n",
    "        \n",
    "        # Check if child node already mapped, otherwise add \n",
    "        if not bool([i for i in nodes if nodes[i][0] == child_node_id]):\n",
    "            nodes[counter]  = [row['ltable_Clean_Name'] if child_node_id != 'NFL' else ''\n",
    "                               , child_node_id if child_node_id != 'NFL' else ''\n",
    "                               , row['ltable_Age'] if (child_node_id != 'NFL' and row['ltable_Age'] == row['ltable_Age']) else ''\n",
    "                               , row['ltable_Birthday'] if (child_node_id != 'NFL' and row['ltable_Birthday'] == row['ltable_Birthday']) else ''\n",
    "                               , row['ltable_Current Status'] if child_node_id != 'NFL' else ''\n",
    "                               , row['ltable_College'] if child_node_id != 'NFL' else ''\n",
    "                               , row['ltable_Clean_Name'] if child_node_id != 'NFL' else ''\n",
    "                              ]\n",
    "            counter+=1                   \n",
    "            \n",
    "    # write nodes CSV file \n",
    "    with open(node_file, 'w',  newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(node_header)\n",
    "        for node in nodes:\n",
    "            if (nodes[node][0] == 'NFL'):\n",
    "                writer.writerow([node, nodes[node][0], nodes[node][0],'','','','', nodes[node][6]])\n",
    "            else:\n",
    "                writer.writerow([node, nodes[node][0], nodes[node][1],nodes[node][2],nodes[node][3],nodes[node][4],nodes[node][5],nodes[node][6]])\n",
    "                \n",
    "    # compute execution time        \n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    return nodes, exec_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692c6d2d-0882-45d8-beb8-2207d40f73ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set data dump path for Neo4j \n",
    "neo4j_data_path = \"/Users/camm/Library/NEO4J_HOME/import\"\n",
    "\n",
    "# Construct Node CSV file\n",
    "node_map, exec_time = processNodes(ab_fullname_cand.copy(), neo4j_data_path+\"/CTE_Nodes.csv\") \n",
    "print(\"Exec time --- %s seconds ---\" % exec_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dfc6119-c05e-4185-967a-77d472bebb86",
   "metadata": {},
   "source": [
    "### Construct Neo4j Relations CSV File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8834882-7aa1-4c1d-b1d9-8ab95a237daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def processRelations(data, nodes, rel_file):\n",
    "    relation_header = [\":START_ID\",\":END_ID\",\":TYPE\"]\n",
    "    relation_data = []\n",
    "    \n",
    "    # Set start time to calculate compute time\n",
    "    start_time = time.time() \n",
    "    \n",
    "     # Construct relation map:\n",
    "    for index, row in data.iterrows(): \n",
    "        if (row.direction == 'parent_to_child'):                \n",
    "            # relation_data.append([nodes[list(nodes.keys()) [list(nodes.values()).index(row.parent)]], \n",
    "            #                       nodes[list(nodes.keys()) [list(nodes.values()).index(row['ltable_Player Id'])]], \n",
    "            #                       row['rtable_cte_category']]) \n",
    "            \n",
    "            relation_data.append([[i for i in nodes if nodes[i][1] == row.parent][0], \n",
    "                                  [i for i in nodes if nodes[i][1] == row['ltable_Player Id']][0], \n",
    "                                  row['rtable_cte_category']])             \n",
    "        else:\n",
    "            # relation_data.append([nodes[list(node_map.keys()) [list(nodes.values()).index(row['ltable_Player Id'])]], \n",
    "            #                       nodes[list(node_map.keys()) [list(nodes.values()).index(row.parent)]], \n",
    "            #                       row['rtable_cte_category']]) \n",
    "\n",
    "            relation_data.append([[i for i in nodes if nodes[i][0] == row['ltable_Player Id']][0],\n",
    "                                 [i for i in nodes if nodes[i][0] == row.parent][0],\n",
    "                                  row['rtable_cte_category']])    \n",
    "            \n",
    "    # wirte relation file \n",
    "    with open(rel_file, 'w',  newline='') as f:\n",
    "        writer = csv.writer(f)\n",
    "        writer.writerow(relation_header)\n",
    "        writer.writerows(relation_data)         \n",
    "            \n",
    "    # compute execution time        \n",
    "    exec_time = time.time() - start_time\n",
    "    \n",
    "    return exec_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "598b9c88-b464-455c-854c-99d7af57edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct relation 'cites' & 'family-cites' CSV file\n",
    "exec_time = processRelations(ab_fullname_cand.copy(), node_map, neo4j_data_path+\"/CTE_Relations.csv\")\n",
    "print(\"Exec time --- %s seconds ---\" % exec_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc747588-7cc3-4eff-acb5-f0f03a45fb21",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293aec93-c9ba-418c-a51f-2fd7e9801cbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5668744c-3a85-48e8-b4de-e8d323d5a6ae",
   "metadata": {},
   "source": [
    "## Text to Knowledge Graph - Demo Example for Group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24799202-ff7c-485b-9c01-bfd37012e45a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlalchemy, psycopg2\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from spacy.matcher import Matcher \n",
    "from spacy.tokens import Span \n",
    "\n",
    "import networkx as nx\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "%matplotlib inline\n",
    "\n",
    "sentences = nltk.sent_tokenize(wiki_page_object.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b654190b-3c16-4499-a2b1-557c647cefa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------\n",
    "# Change below creds/config per your own DB settings:\n",
    "# ----------------------------------------------------\n",
    "db_host     = 'awesome-hw.sdsc.edu'   # <- enter your DB host name\n",
    "db_name     = 'postgres'              # <- enter your DB name\n",
    "db_username = 'ag_class'              # <- enter your DB username\n",
    "db_password = 'WUcgdfQ1'              # <- enter your DB password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ec1987-fbf5-4dfe-99a3-9d351927906c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a postgresql engine instance\n",
    "print('Connection string: postgresql://' + db_username +':' + db_password + '@' + db_host + '/' + db_name)\n",
    "alchemyEngine  = sqlalchemy.create_engine('postgresql://' + db_username +':' + db_password + '@' + db_host + '/' + db_name)\n",
    "%reload_ext sql\n",
    "%sql $alchemyEngine.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52dc21d3-ede1-4d89-8b23-c0f0de01bcab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set start time to calculate compute time\n",
    "start_time = time.time()\n",
    "\n",
    "# Connect to PostgreSQL server\n",
    "dbConnection    = alchemyEngine.connect();\n",
    "\n",
    "# SQL command\n",
    "sql = \"\"\"\n",
    "        SELECT DISTINCT title, news, keywords from usnewspaper WHERE ARRAY['cte','lawsuit']::text[] <@ keywords AND news IS NOT NULL\n",
    "        UNION\n",
    "        /*\n",
    "        SELECT DISTINCT title, news, keywords  from usnewspaper WHERE ARRAY['nfl', 'helmet']::text[] <@ keywords AND news IS NOT NULL\n",
    "        UNION\n",
    "        SELECT DISTINCT title, news, keywords  from usnewspaper WHERE ARRAY['nfl', 'brain']::text[] <@ keywords AND news IS NOT NULL\n",
    "        UNION */\n",
    "        SELECT DISTINCT title, news, keywords from usnewspaper WHERE ARRAY['encephalopathy']::text[] <@ keywords \n",
    "             AND news IS NOT NULL AND title ilike '%nfl%';\n",
    "      \"\"\"\n",
    "\n",
    "# Read data from PostgreSQL database table and load into a DataFrame instance\n",
    "news_data_df = pd.read_sql_query(sqlalchemy.text(sql), alchemyEngine)\n",
    "\n",
    "# Close the database connection\n",
    "dbConnection.close();\n",
    "\n",
    "pd.set_option('display.expand_frame_repr', False);\n",
    "\n",
    "print(\"Exec time --- %s seconds ---\" % (time.time() - start_time)) \n",
    "\n",
    "# Display the DataFrame\n",
    "display(news_data_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34f3e5fa-c5bf-4890-b856-27d6cc936e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetEntities(sent):\n",
    "    ## chunk 1\n",
    "    ent1 = \"\"\n",
    "    ent2 = \"\"\n",
    "\n",
    "    prv_tok_dep = \"\"    # dependency tag of previous token in the sentence\n",
    "    prv_tok_text = \"\"   # previous token in the sentence\n",
    "\n",
    "    prefix = \"\"\n",
    "    modifier = \"\"\n",
    "\n",
    "  #############################################################\n",
    "  \n",
    "    for tok in nlp(sent):\n",
    "    ## chunk 2\n",
    "        # # if token is a punctuation mark then move on to the next token\n",
    "        # if tok.dep_ != \"punct\":\n",
    "        \n",
    "        # check: token is a compound word or not\n",
    "        if tok.dep_ == \"compound\":\n",
    "            prefix = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                prefix = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "        # check: token is a modifier or not\n",
    "        if tok.dep_.endswith(\"mod\") == True:\n",
    "            modifier = tok.text\n",
    "            # if the previous word was also a 'compound' then add the current word to it\n",
    "            if prv_tok_dep == \"compound\":\n",
    "                modifier = prv_tok_text + \" \"+ tok.text\n",
    "      \n",
    "        ## chunk 3\n",
    "        if tok.dep_.find(\"subj\") == True:\n",
    "            ent1 = modifier +\" \"+ prefix + \" \"+ tok.text\n",
    "            prefix = \"\"\n",
    "            modifier = \"\"\n",
    "            prv_tok_dep = \"\"\n",
    "            prv_tok_text = \"\"      \n",
    "\n",
    "        ## chunk 4\n",
    "        if tok.dep_.find(\"obj\") == True:\n",
    "            ent2 = modifier +\" \"+ prefix +\" \"+ tok.text\n",
    "        \n",
    "        ## chunk 5  \n",
    "        # update variables\n",
    "        prv_tok_dep = tok.dep_\n",
    "        prv_tok_text = tok.text\n",
    "  #############################################################\n",
    "\n",
    "    return [ent1.strip(), ent2.strip()]\n",
    "\n",
    "\n",
    "def GetRelation(sent):\n",
    "\n",
    "    doc = nlp(sent)\n",
    "\n",
    "    # Matcher class object \n",
    "    matcher = Matcher(nlp.vocab)\n",
    "\n",
    "    #define the pattern \n",
    "    pattern = [{'DEP':'ROOT'}, \n",
    "            {'DEP':'prep','OP':\"?\"},\n",
    "            {'DEP':'agent','OP':\"?\"},  \n",
    "            {'POS':'ADJ','OP':\"?\"}]\n",
    "        \n",
    "    matcher.add(\"matching_1\", [pattern],  greedy='LONGEST') \n",
    "\n",
    "    matches = matcher(doc)\n",
    "    k = len(matches) - 1\n",
    "\n",
    "    span = doc[matches[k][1]:matches[k][2]] \n",
    "\n",
    "    return(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc2b249-deab-4dfc-a0ef-b19fef12cb2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = nltk.sent_tokenize(RemoveStopWords(re.sub(\"\\n|\\r\", \" \", news_data_df['news'].replace(r'[^\\W\\s]+(?<!.)', '', regex=True)[1].replace(\"“\",\"\").replace(\"”\",\"\").replace(\"’\",\"\").replace(\"'\",\"\").lower())))\n",
    "sentences[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5389e2d1-2e09-431e-b399-a47c776a214b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0,len(sentences)):\n",
    "    if(GetEntities(str(sentences[i]))[0] != '' and GetEntities(str(sentences[i]))[1] !=''):\n",
    "        print(f\"Ents: {GetEntities(str(sentences[i]))} \\t\\t\\tRels: {GetRelation(str(sentences[i]))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cfd938b-4d3f-4eaa-a646-a7a6ce6f2429",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c707690d-bd18-49cb-b384-3797c3ea3e8f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
